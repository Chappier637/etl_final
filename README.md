# Отчет
Скрины и приложенные файлы см. в папках, соответствующих заданиям

## Задание 1

### Что было сделано
1. Подготовка инфраструктуры
   - Созадл сервис YDB
   - Создал сервис Data Transfer
   - Создал сервисный аккаунт с правами storage.editor, ydb.editor
2. Создание таблицы в YDB и заливка данных туда (см. приложенные скрипты sql и sh. Ссылка на данные в файле readme.md)
3. Настройка трансфера
   - Создал эндпоинт для приемника Object Storage
   - Создал эндпоинт для источника YDB
   - Создал транфер и активировал его
     
### Что получилось
Все успешно отработало (см. скрин)

## Задание 2

### Что было сделано
1. Подготовка инфраструктуры
   - Создал сервисный аккаунт с ролями dataproc.editor и editor
   - Создал бакет в OS для выполнения задания
   - Создал сеть с подсетью в регионе ru-central1-a и настроил для этой подсети NAT-шлюз
   - Создал кластер Metastore
   - Создал кластер Airflow
2. Подготовка PySpark-задания
   - Локально создал файл create-table.py (см. в папке)
   - Создал в бакете папку scripts и залил в ее созданный файл
3. Подготовка и запуск дага
   - Локально создал SSH-ключ
   - Локально создал файл Data-Processing-Dag.py (cv. в папке)
   - Создал в бакете папку dags и закинул в нее созданный файл
   - Запустил даг в веб-интерфейсе Airflow

### Что получилось
Даг успешно отработал и выполнил PySpark-задание (см. скрин в папке). Данные залились в бакет (см. скрин и ссылку на данные в папке)

## Задание 3

### Что было сделано
1. Подготовка инфраструктуры
   - Создал сеть, в ней создал подсеть
   - Настроил для созданной подсети NAT-шлюз
   - Создал группу безопасности в сети и настроил ее
   - Создал сервисный аккаунт с правами storage.viewer, storage.uploader, dataproc.user и dataproc.agent
   - Создал бакет в OS и дал сервисному аккаунту FULL_CONTROL доступ
   - Создал кластер Data Processiong c сервисами HDFS, YARN, SPARK, TEZ и LIVY
   - Создал кластер Kafka
   - В кластере Kafka создал топик и пользователя со всеми правами на все топики
2. Подготовка PySpark заданий
   - Подготовил файл для записи сообщений в Kafka (см. kafka-write.py)
   - Подготовил файл для потокового чтения из Kafka (см. kafka-read-stream.py)
   - Закинул в корень бакета созданные скрипты
   - Запустил в Data Processiong PySpark-задания (сначала на запись, потом на чтение) (см. скрины в папке задания)

### Что получилось
Сообщения записались в kafka первым скриптом, прочитались вторым и залились в бакет (см. скрины и txt-файлы в папке)
